{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import GCNConv\n",
    "import random,os\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import scipy.sparse as sp\n",
    "import networkx as nx\n",
    "import dgl\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch_geometric.nn import GCN\n",
    "from pygod.nn.decoder import DotProductDecoder\n",
    "from pygod.nn.functional import double_recon_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=3407):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "set_seed()\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nhid)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.relu(self.gc2(x, adj))\n",
    "\n",
    "        return x\n",
    "\n",
    "class Attribute_Decoder(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, dropout):\n",
    "        super(Attribute_Decoder, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nhid, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nfeat)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.relu(self.gc2(x, adj))\n",
    "\n",
    "        return x\n",
    "\n",
    "class Structure_Decoder(nn.Module):\n",
    "    def __init__(self, nhid, dropout):\n",
    "        super(Structure_Decoder, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nhid, nhid)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        # x = F.sigmoid(x @ x.T)\n",
    "        x_norm= torch.norm(x, p=2, dim=1, keepdim=True)\n",
    "        x = x/x_norm\n",
    "        x[torch.isnan(x)] = 0\n",
    "        return x@x.T\n",
    "        # return x\n",
    "\n",
    "class RE_Reconstruction(nn.Module):\n",
    "    def __init__(self, feat_size, hidden_size, dropout):\n",
    "        super(Dominant, self).__init__()\n",
    "        \n",
    "        self.attr_encoder = Encoder(feat_size, hidden_size, dropout)\n",
    "        self.struct_encoder = Encoder(feat_size, hidden_size, dropout)\n",
    "        self.attr_decoder = Attribute_Decoder(feat_size, hidden_size, dropout)\n",
    "        self.struct_decoder = Structure_Decoder(hidden_size, dropout)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        # encode\n",
    "        x1 = self.attr_encoder(x, adj)\n",
    "        x2 = self.struct_encoder(x, adj)\n",
    "        # decode feature matrix\n",
    "        x_hat = self.attr_decoder(x1, adj)\n",
    "        # decode adjacency matrix\n",
    "        struct_reconstructed = self.struct_decoder(x2, adj)\n",
    "        # return reconstructed matrices\n",
    "        return struct_reconstructed, x_hat, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "\n",
    "ds = 'reddit'\n",
    "set_seed()\n",
    "graph = dgl.load_graphs('/data/zhengling/datasets/{}/{}'.format(ds, ds))[0][0]\n",
    "device = 'cuda:0'\n",
    "\n",
    "x = graph.ndata['feature'].to(torch.float32).to(device)\n",
    "num_nodes = x.shape[0]\n",
    "y = graph.ndata['label'].detach().numpy()\n",
    "edges = graph.edges()\n",
    "edges = [e.tolist() for e in edges]\n",
    "edges = torch.LongTensor(edges).to(device)\n",
    "adj_true = to_dense_adj(edges, max_num_nodes=num_nodes)[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygod.nn.functional import double_recon_loss\n",
    "set_seed()\n",
    "\n",
    "def loss_func(adj, A_hat, attrs, X_hat, src_nodes, dst_nodes, norm_index, anom_index, alpha, beta):\n",
    "    # Attribute reconstruction loss\n",
    "    diff_attribute = torch.pow(X_hat - attrs, 2)\n",
    "    x_diff = X_hat - attrs\n",
    "    attribute_reconstruction_errors = torch.sqrt(torch.sum(diff_attribute, 1))\n",
    "    # norm_attribute_errors = torch.mean(torch.sqrt(torch.sum(diff_attribute[norm_index], 1))).detach().cpu().numpy()\n",
    "    # anom_attribute_errors = torch.mean(torch.sqrt(torch.sum(diff_attribute[anom_index], 1))).detach().cpu().numpy()\n",
    "    attribute_cost = torch.mean(attribute_reconstruction_errors)\n",
    "\n",
    "    # structure reconstruction loss\n",
    "    diff_structure = torch.pow(A_hat - adj, 2)\n",
    "    a_diff = A_hat - adj\n",
    "    structure_reconstruction_errors = torch.sqrt(torch.sum(diff_structure, 1))\n",
    "    # norm_structure_errors = torch.mean(torch.sqrt(torch.sum(diff_structure[norm_index], 1))).detach().cpu().numpy()\n",
    "    # anom_structure_errors = torch.mean(torch.sqrt(torch.sum(diff_structure[anom_index], 1))).detach().cpu().numpy()\n",
    "    structure_cost = torch.mean(structure_reconstruction_errors)\n",
    "\n",
    "    src_x = attrs[src_nodes]\n",
    "    dst_x = attrs[dst_nodes]\n",
    "    src_x_norm = src_x / (torch.norm(src_x, dim=1, keepdim=True) + 1e-8)\n",
    "    dst_x_norm = dst_x / (torch.norm(dst_x, dim=1, keepdim=True) + 1e-8)\n",
    "    cos_sim_x = torch.sum(src_x_norm * dst_x_norm, dim=1)\n",
    "    \n",
    "    src_hat = X_hat[src_nodes]\n",
    "    dst_hat = X_hat[dst_nodes]\n",
    "    src_hat_norm = src_hat / (torch.norm(src_hat, dim=1, keepdim=True) + 1e-8)\n",
    "    dst_hat_norm = dst_hat / (torch.norm(dst_hat, dim=1, keepdim=True) + 1e-8)\n",
    "    cos_sim_hat = torch.sum(src_hat_norm * dst_hat_norm, dim=1)\n",
    "\n",
    "    diff_sim = torch.pow(cos_sim_x - cos_sim_hat, 2)\n",
    "    sim_reconstruction_errors = torch.sqrt(torch.sum(diff_sim))\n",
    "    sim_cost = torch.mean(sim_reconstruction_errors)\n",
    "    \n",
    "    \n",
    "    sim_reconstruction_errors = torch.sqrt(torch.sum(diff_sim))\n",
    "    sim_cost = torch.mean(sim_reconstruction_errors)\n",
    "\n",
    "    cost =  alpha * attribute_reconstruction_errors + beta * structure_reconstruction_errors + (1 - alpha - beta) * sim_reconstruction_errors\n",
    "    diff_feats = torch.cat((x_diff, a_diff), dim=1)\n",
    "\n",
    "    # return cost, structure_cost, attribute_cost, diff_feats, norm_attribute_errors, norm_structure_errors, anom_attribute_errors, anom_structure_errors\n",
    "    # return cost, structure_cost, attribute_cost, norm_attribute_errors, norm_structure_errors, anom_attribute_errors, anom_structure_errors\n",
    "    return cost, structure_cost, attribute_cost, sim_cost, diff_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = torch.tensor(graph.ndata['train_masks'][:,0]).bool().to(device)\n",
    "print(train_mask.shape)\n",
    "valid_mask = torch.tensor(graph.ndata['val_masks'][:,0]).bool().to(device)\n",
    "test_mask = torch.tensor(graph.ndata['test_masks'][:, 0]).bool().to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_nodes = edges[0]\n",
    "dst_nodes = edges[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = edges\n",
    "adj_label = adj_true\n",
    "attrs = x\n",
    "set_seed()\n",
    "\n",
    "epoch_num = 100\n",
    "alphas = [i * 0.1 for i in range(11)]\n",
    "y = graph.ndata['label'].to(device)\n",
    "# alphas = [0.8]\n",
    "# betas = [0.0]\n",
    "betas = [i * 0.1 for i in range(11)]\n",
    "losses = []\n",
    "diff_norm_feats = []\n",
    "diff_norm_adjs = []\n",
    "diff_anom_feats = []\n",
    "diff_anom_adjs = []\n",
    "best_valid_auc = -1\n",
    "max_aucs = []\n",
    "for alpha in alphas:\n",
    "    for beta in betas: \n",
    "        if alpha + beta > 1: continue\n",
    "        model = RE_Reconstruction(feat_size = attrs.size(1), hidden_size = 256, dropout = 0.3).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = 5e-4)\n",
    "        train_aucs = []\n",
    "        valid_aucs = []\n",
    "        test_aucs = []\n",
    "        all_aucs = []\n",
    "        for epoch in range(epoch_num):\n",
    "            auc = 0\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            A_hat, X_hat, latent_x = model(attrs, adj_true)\n",
    "            # loss, struct_loss, feat_loss, diff_feats, diff_norm_feat, diff_norm_adj, diff_anom_feat, diff_anom_adj = loss_func(adj_label, A_hat, attrs, X_hat, alpha, y==0, y==1)\n",
    "            # loss, struct_loss, feat_loss, diff_norm_feat, diff_norm_adj, diff_anom_feat, diff_anom_adj = loss_func(adj_label, A_hat, attrs, X_hat, alpha, y==0, y==1)\n",
    "            loss, struct_loss, feat_loss, sim_loss, diff_feats, = loss_func(adj_label, A_hat, attrs, X_hat, src_nodes, dst_nodes, y==0, y==1, alpha, beta)\n",
    "            # print(diff_anom_adj, torch.mean(diff_anom_adj))\n",
    "            # loss, diff_feats = loss_func(adj_label, A_hat, attrs, X_hat, alpha)\n",
    "            l = torch.mean(loss)\n",
    "            l.backward()\n",
    "            optimizer.step()  \n",
    "            losses.append(l.detach().cpu())\n",
    "            # diff_norm_feats.append(diff_norm_feat)\n",
    "            # diff_norm_adjs.append(diff_norm_adj)\n",
    "            # diff_anom_feats.append(diff_anom_feat)\n",
    "            # diff_anom_adjs.append(diff_anom_adj)      \n",
    "\n",
    "            if (epoch+1)%1 == 0 or epoch == epoch_num - 1:\n",
    "                if (epoch + 1) % 10 == 0 or epoch == epoch_num-1:\n",
    "                    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(l.item()), \"train/struct_loss=\", \"{:.5f}\".format(struct_loss.item()),\"train/feat_loss=\", \"{:.5f}\".format(feat_loss.item()), \"train/sim_loss=\", \"{:.5f}\".format(sim_loss.item()))\n",
    "                model.eval()\n",
    "                A_hat, X_hat, latent_x = model(attrs, adj_true)\n",
    "                # loss, struct_loss, feat_loss, diff_feats, diff_norm_feat, diff_norm_adj, diff_anom_feat, diff_anom_adj = loss_func(adj_label, A_hat, attrs, X_hat, alpha, y==0, y==1)\n",
    "                loss, struct_loss, feat_loss, sim_loss, diff_feats = loss_func(adj_label, A_hat, attrs, X_hat, src_nodes, dst_nodes, y==0, y==1, alpha, beta)\n",
    "                # loss, diff_feats = loss_func(adj_label, A_hat, attrs, X_hat, alpha)\n",
    "                auc = roc_auc_score(y[train_mask].detach().cpu().numpy(), loss[train_mask].detach().cpu().numpy())\n",
    "                # print(\"Epoch:\", '%04d' % (epoch + 1), 'train: Auc', auc)\n",
    "                # print(torch.where(X_hat > 0))\n",
    "                train_aucs.append(auc)\n",
    "                v_a = roc_auc_score(y[valid_mask].detach().cpu().numpy(), loss[valid_mask].detach().cpu().numpy())\n",
    "                # print(\"Epoch:\", '%04d' % (epoch + 1), 'valid: Auc', v_a)\n",
    "                valid_aucs.append(v_a)\n",
    "                if v_a > best_valid_auc:\n",
    "                    best_valid_auc = v_a\n",
    "                    torch.save(model.state_dict(), 'checkpoint_' + ds + '.pt')\n",
    "                test_auc = roc_auc_score(y[test_mask].detach().cpu().numpy(), loss[test_mask].detach().cpu().numpy())\n",
    "                all_auc = roc_auc_score(y.detach().cpu().numpy(), loss.detach().cpu().numpy())\n",
    "                all_aucs.append(all_auc)\n",
    "                test_aucs.append(test_auc)\n",
    "        print('alpha : ', alpha, 'beta: ', beta,  max(train_aucs), max(valid_aucs), max(test_aucs), max(all_aucs))\n",
    "        print('valid_aucs: ', valid_aucs)\n",
    "        print('test_aucs: ', test_aucs)\n",
    "        max_aucs.append([alpha, beta, max(train_aucs), max(valid_aucs), max(test_aucs), max(all_aucs)])\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aucs = [i[2] for i in max_aucs ]\n",
    "valid_aucs = [i[3] for i in max_aucs ]\n",
    "test_aucs = [i[4] for i in max_aucs ]\n",
    "all_aucs = [i[5] for i in max_aucs]  \n",
    "print(max(train_aucs), max(valid_aucs), max(test_aucs), max(all_aucs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "# model = Dominant(feat_size = 10, hidden_size = 256, dropout = 0.3).to(device)\n",
    "model = RE_Reconstruction(feat_size = attrs.size(1), hidden_size = 256, dropout = 0.3).to(device)\n",
    "model.load_state_dict(torch.load('checkpoint_' + ds + '.pt'))\n",
    "model.eval()\n",
    "A_hat, X_hat, latent_x = model(attrs, adj_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "loss, struct_loss, feat_loss, sim_loss, diff_feats = loss_func(adj_label, A_hat, attrs, X_hat, src_nodes, dst_nodes, y==0, y==1, 0.8, 0)\n",
    "print(roc_auc_score(y[test_mask].detach().cpu().numpy(), loss[test_mask].detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min = torch.min(loss)\n",
    "max = torch.max(loss)\n",
    "\n",
    "loss_score = (loss-min)/(max-min)\n",
    "print(loss_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于 latent_x, X_hat-x, A_hat-A, sim_hat - sim 进行聚类\n",
    "# 或者只是基于 loss 进行一个排序呢？\n",
    "new_sim_matrix = np.zeros([num_nodes, num_nodes])\n",
    "print(new_sim_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_scores = loss.detach().cpu().numpy()\n",
    "anomaly_scores_index = np.argsort(anomaly_scores)\n",
    "anomaly_scores_sorted = np.sort(anomaly_scores)\n",
    "print(anomaly_scores_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "edges_for_nx = edges.detach().cpu().numpy().T\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(edges_for_nx)\n",
    "print(G.number_of_edges(), G.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不能说 异常程度相近的就算异常\n",
    "# 通过 latent_x, diff_pattern 的相似度\n",
    "latent_x_sim = latent_x@latent_x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_feats = X_hat - attrs\n",
    "diff_feats_sim = diff_feats@diff_feats.T\n",
    "print(diff_feats_sim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in [a/10 for a in range(11)]:\n",
    "    tmp_sim_matrix = r * latent_x_sim + (1-r) * diff_feats_sim\n",
    "    print(tmp_sim_matrix.shape)\n",
    "    tmp_sim_matrix = tmp_sim_matrix.detach().cpu().numpy()\n",
    "    cnt = 0\n",
    "    cnt1 = 0\n",
    "    nearest_index = {}\n",
    "    for i in range(num_nodes):\n",
    "        nearest_index[i] = np.where(np.argsort(tmp_sim_matrix[i])==num_nodes-1)\n",
    "        if y[i]== y[nearest_index[i]]:\n",
    "            cnt += 1\n",
    "        if G.has_edge(i,y[nearest_index[i]]) or G.has_edge(y[nearest_index[i]], i):\n",
    "            cnt1 += 1\n",
    "    print(cnt, cnt1)\n",
    "    new_edges = [[], []]\n",
    "    for i in nearest_index:\n",
    "        new_edges[0].append(i)\n",
    "        new_edges[1].append(nearest_index[i][0][0])\n",
    "    new_edges = torch.tensor(new_edges).to(device)\n",
    "    final_edges = torch.cat([edges, new_edges], dim=1)\n",
    "    new_graph = dgl.graph((final_edges[0].detach().cpu().numpy(), final_edges[1].detach().cpu().numpy()))\n",
    "    new_graph.ndata['train_mask'] = graph.ndata['train_masks']\n",
    "    new_graph.ndata['val_mask'] = graph.ndata['val_masks']\n",
    "    new_graph.ndata['test_mask'] = graph.ndata['test_masks']\n",
    "    # graph.ndata['mark'] = torch.tensor(marks).bool()\n",
    "    new_graph.ndata['label'] = y.cpu()\n",
    "    new_graph.ndata['feature'] = attrs.cpu()\n",
    "    new_graph.ndata['label_1'] = loss_score.cpu()\n",
    "\n",
    "    dgl.save_graphs('datasets/new_' + ds + '_' + str(int(r*10)), [new_graph])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
